{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e80f5e1-9712-44d8-9718-db697fb0b042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob \n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b3f83fb-c361-4a89-8a6e-0878855accd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-ollama in /home/kaan/anaconda3/lib/python3.13/site-packages (0.3.8)\n",
      "Requirement already satisfied: ollama<1.0.0,>=0.5.3 in /home/kaan/anaconda3/lib/python3.13/site-packages (from langchain-ollama) (0.6.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.76 in /home/kaan/anaconda3/lib/python3.13/site-packages (from langchain-ollama) (0.3.76)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in /home/kaan/anaconda3/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.76->langchain-ollama) (0.4.31)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /home/kaan/anaconda3/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.76->langchain-ollama) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/kaan/anaconda3/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.76->langchain-ollama) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/kaan/anaconda3/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.76->langchain-ollama) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/kaan/anaconda3/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.76->langchain-ollama) (4.12.2)\n",
      "Requirement already satisfied: packaging>=23.2 in /home/kaan/anaconda3/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.76->langchain-ollama) (24.2)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in /home/kaan/anaconda3/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.76->langchain-ollama) (2.10.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/kaan/anaconda3/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.76->langchain-ollama) (2.1)\n",
      "Requirement already satisfied: httpx>=0.27 in /home/kaan/anaconda3/lib/python3.13/site-packages (from ollama<1.0.0,>=0.5.3->langchain-ollama) (0.28.1)\n",
      "Requirement already satisfied: anyio in /home/kaan/anaconda3/lib/python3.13/site-packages (from httpx>=0.27->ollama<1.0.0,>=0.5.3->langchain-ollama) (4.7.0)\n",
      "Requirement already satisfied: certifi in /home/kaan/anaconda3/lib/python3.13/site-packages (from httpx>=0.27->ollama<1.0.0,>=0.5.3->langchain-ollama) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /home/kaan/anaconda3/lib/python3.13/site-packages (from httpx>=0.27->ollama<1.0.0,>=0.5.3->langchain-ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/kaan/anaconda3/lib/python3.13/site-packages (from httpx>=0.27->ollama<1.0.0,>=0.5.3->langchain-ollama) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in /home/kaan/anaconda3/lib/python3.13/site-packages (from httpcore==1.*->httpx>=0.27->ollama<1.0.0,>=0.5.3->langchain-ollama) (0.16.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /home/kaan/anaconda3/lib/python3.13/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-ollama) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /home/kaan/anaconda3/lib/python3.13/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-ollama) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /home/kaan/anaconda3/lib/python3.13/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-ollama) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /home/kaan/anaconda3/lib/python3.13/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-ollama) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/kaan/anaconda3/lib/python3.13/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain-ollama) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /home/kaan/anaconda3/lib/python3.13/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain-ollama) (2.27.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/kaan/anaconda3/lib/python3.13/site-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-ollama) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/kaan/anaconda3/lib/python3.13/site-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-ollama) (2.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/kaan/anaconda3/lib/python3.13/site-packages (from anyio->httpx>=0.27->ollama<1.0.0,>=0.5.3->langchain-ollama) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3e7652f-b4fe-4637-9acc-2a8814bf87df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.chat_models import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccfc696b-8caa-49f5-a35e-de3b60ee5a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"llama3.2\"\n",
    "db_name = \"vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a803526-cde1-4ed7-ab19-b0668fd9db23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1088, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "folders = glob.glob(\"knowledge-base/*\")\n",
    "\n",
    "def add_metadata(doc, doc_type):\n",
    "    doc.metadata[\"doc_type\"] = doc_type\n",
    "    return doc\n",
    "\n",
    "documents = []\n",
    "for folder in folders:\n",
    "    doc_type = os.path.basename(folder)\n",
    "    loader = DirectoryLoader(folder, glob = \"**/*.md\", loader_cls = TextLoader)\n",
    "    folder_docs = loader.load()\n",
    "    documents.extend([add_metadata(doc, doc_type) for doc in folder_docs])\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73d71137-e0bb-44a2-9fbb-12ad0b3d55bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72eb1637-f017-404c-915e-ed65bc06bbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name = BERT_MODEL_NAME,\n",
    "    model_kwargs = {\"device\" : \"cuda\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "439799a1-c1ff-4a62-a740-e6206be5712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f27df880-07d4-4786-84b5-e949b78204f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model = model, temperature = 0.7)\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key = 'chat_history', return_messages = True) \n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever = retriever, memory = memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df0782f6-1175-472f-a09b-8813169facfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    result = conversation_chain.invoke({\"question\":message})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb4e2fd7-19d5-4bff-bcc6-9baa769a9a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaan/anaconda3/lib/python3.13/site-packages/gradio/chat_interface.py:347: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view = gr.ChatInterface(chat).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "012f04bb-2bf9-4ea1-a592-8b9b172b4e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "## Annual Performance History\n",
      "- **2020:**  \n",
      "  - Completed onboarding successfully.  \n",
      "  - Met expectations in delivering project milestones.  \n",
      "  - Received positive feedback from the team leads.\n",
      "\n",
      "- **2021:**  \n",
      "  - Achieved a 95% success rate in project delivery timelines.  \n",
      "  - Awarded \"Rising Star\" at the annual company gala for outstanding contributions.  \n",
      "\n",
      "- **2022:**  \n",
      "  - Exceeded goals by optimizing existing backend code, improving system performance by 25%.  \n",
      "  - Conducted training sessions for junior developers, fostering knowledge sharing.  \n",
      "\n",
      "- **2023:**  \n",
      "  - Led a major overhaul of the API internal architecture, enhancing security protocols.  \n",
      "  - Contributed to the company’s transition to a cloud-based infrastructure.  \n",
      "  - Received an overall performance rating of 4.8/5.\n",
      "\n",
      "## Annual Performance History\n",
      "- **2023:** Rating: 4.5/5  \n",
      "  *Samuel exceeded expectations, successfully leading a cross-departmental project on AI-driven underwriting processes.*\n",
      "\n",
      "- **2022:** Rating: 3.0/5  \n",
      "  *Some challenges in meeting deadlines and collaboration with the engineering team. Received constructive feedback and participated in a team communication workshop.*\n",
      "\n",
      "- **2021:** Rating: 4.0/5  \n",
      "  *There was notable improvement in performance. Worked to enhance model accuracy, leading to improved risk assessment outcomes for B2C customers.*\n",
      "\n",
      "- **2020:** Rating: 3.5/5  \n",
      "  *Exhibited a solid performance during the initial year as a Senior Data Scientist but had struggles adapting to new leadership expectations.*\n",
      "\n",
      "## Compensation History\n",
      "- **2023:** Base Salary: $115,000 + Bonus: $15,000  \n",
      "  *Annual bonus based on successful project completions and performance metrics.*\n",
      "\n",
      "## Annual Performance History\n",
      "- **2018**: **3/5** - Adaptable team player but still learning to take initiative.\n",
      "- **2019**: **4/5** - Demonstrated strong problem-solving skills, outstanding contribution on the claims project.\n",
      "- **2020**: **2/5** - Struggled with time management; fell behind on deadlines during a high-traffic release period.\n",
      "- **2021**: **4/5** - Made a significant turnaround with organized work habits and successful project management.\n",
      "- **2022**: **5/5** - Exceptional performance during the \"Innovate\" initiative, showcasing leadership and creativity.\n",
      "- **2023**: **3/5** - Maintaining steady work; expectations for innovation not fully met, leading to discussions about goals.\n",
      "\n",
      "## Annual Performance History\n",
      "- **2019:** Exceeds Expectations - Continuously delivered high-quality code and participated actively in team meetings.\n",
      "- **2020:** Meets Expectations - Jordan K. Bishop maintained steady performance but faced challenges due to a higher workload from multiple projects.\n",
      "- **2021:** Exceeds Expectations - Recognized for leadership during the customer portal project; received the “Innovation Award” for creative problem-solving.\n",
      "- **2022:** Meets Expectations - While mentoring others, the shift in focus led to fewer contributions to new features, marking a decrease in performance.\n",
      "- **2023:** Needs Improvement - Transitioning back to development has resulted in difficulties with recent technologies, prompting a performance improvement plan.\n",
      "Human: Who received the prestigious IIOTY award in 2023?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Answer:  I don't know who received the prestigious IIOTY (Investor of the Year) award in 2023. The provided text does not mention an IIOTY award or any awards related to this topic.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.callbacks import StdOutCallbackHandler\n",
    "\n",
    "llm = ChatOllama(model = model, temperature = 0.7)\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key = 'chat_history', return_messages = True)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory, callbacks = [StdOutCallbackHandler()])\n",
    "\n",
    "query = \"Who received the prestigious IIOTY award in 2023?\"\n",
    "result = conversation_chain.invoke({\"question\":query})\n",
    "answer = result[\"answer\"]\n",
    "print(\"\\nAnswer: \", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3923cc08-1a2c-45e3-8a2a-7da43ed9f4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model = model, temperature = 0.7)\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages = True)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs = {\"k\":25})\n",
    "\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever = retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa410bd5-0bc2-4fd3-83d8-e75f71859cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(question, history):\n",
    "    result = conversation_chain.invoke({\"question\": question})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f3f3be0-b4e2-42de-b9ab-c396973109c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaan/anaconda3/lib/python3.13/site-packages/gradio/chat_interface.py:347: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view = gr.ChatInterface(chat).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f4a6f1-2be8-4f4d-8a4a-df3f79c00bf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
